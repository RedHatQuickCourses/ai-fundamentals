# AI Fundamentals



## Introduction


### Lesson overview

This lesson introduces artificial intelligence (AI) and explains why you should care about it.

### Lesson objectives

In this lesson, you will learn how to:

‚Ä¢ Explain, at a high level, the AI market opportunity.

‚Ä¢ Define artificial intelligence, machine learning, and deep learning.

---

### Are you ready to talk about AI?

### üñºÔ∏è Image: Image Content
**Description:** Suppose you were to find yourself in an informal conversation, and talk turns to artificial intelligence. Would you know enough to hold up your end of the conversation?
**üìÅ File Information:**
- **Filename:** stock-image.jpg
- **Location:** scormcontent/assets/stock-image.jpg
- **File Size:** 184.6 KB
- **File Type:** JPG
- **Dimensions:** 1680x1120 pixels
- **Access:** Open SCORM zip ‚Üí scormcontent/assets/stock-image.jpg

You need to be able to discuss AI confidently.

Fortunately, you don'sst need to be a rocket scientist to talk intelligently about AI. And you may not realize it, but you likely interact with AI multiple times a day as you live your life. So relax, take a deep breath, and get started.

---

### Why should you care about AI?

### üñºÔ∏è Image: Image Content
**Description:** So, the million-dollar question: Why is AI so important now?
**üìÅ File Information:**
- **Filename:** stock-image.jpg
- **Location:** scormcontent/assets/79AzwS/stock-image.jpg
- **File Size:** 204.0 KB
- **File Type:** JPG
- **Dimensions:** 1680x943 pixels
- **Access:** Open SCORM zip ‚Üí scormcontent/assets/79AzwS/stock-image.jpg

### Why should you care about AI?

With AI, computers can now understand human language and show some kind of reasoning capabilities AI provides a mechanism for automating the kinds of tasks that were the sole domain of humans The level of complexity is expanding. The machines are getting better and can adapt quickly based on the data they collect.AI is important now because Moore'sss law has continued to apply to the computing industry, and humanity now has enough computing power at its disposal to approximate human intelligence.

Moore'sss law: The number of transistors in an integrated circuit doubles approximately every two years It is named after Gordon Moore, later the co-founder of Intel, who made this prediction in 1975.

Although Moore'sss prediction referred specifically to the number of transistors on a microchip, Moore'sss law has also proven to be accurate with regard to the capabilities of hardware in general and other technology. Advancements such as the reduction in the cost of microprocessors the increase in memory capacity, the improvement of sensors and the number and size of pixels in digital cameras and screens are strongly linked to Moore'sss law.

### What this means for you

Now that technology has improved to the point that AI is viable, AI is changing how we live and work in many ways Many experts have compared the significance of AI and the impact it'ss expected to have to the internet and some believe it will have an even greater impact. Whether you realize it or not, AI is likely already affecting your everyday life, and it is likely to bring changes to your work as well, regardless of your job or role.Organizations across a variety of industries are investing heavily in AI and betting vast sums The next section of the course takes a look at the sheer size of the AI market.

---

### The AI market opportunity

AI is on almost everyone'sss mind right now, and it has rapidly become critical for businesses and organizations AI-related technology has become more readily available and accessible to businesses of all sizes

As a consequence, there is an increase in demand for AI workloads and investments in AI technologies Businesses are ready to use their ever-increasing data to predict business results and automate their operations

### AI is not a 'ssnice to have'ssit'sss a 'ssmust have'ss

Across all verticals AI is being adopted horizontally across all industries

A 2025 survey (https//www.thomsonreuterscom/en-uspostscorporatesc-suite-survey-2025/) of C-suite executives conducted by the Thomson Reuters Institute revealed that 85 believe AI will have a transformational or high impact on their businesses over the next five years

According to recent surveys 78 of enterprise organizations report using AI in at least one business function, an increase from 55 just a year prior. The use of generative AI has been particularly explosive, with 71 of organizations now regularly using it, up from 33 in 2023.

Keep in mind, the aggregate figures mentioned above do not reflect the uneven pattern of adoption across industries company sizes and functional areas

### The market for AI solutions is large and growing larger

It'sss probably no surprise that the AI market is large, but just how large is it?According to a report from Gartner (https//www.gartner.com/en/newsroom/pressreleases2025-09-17-gartner-saysworldwide-ai-spending-will-total-1-point-5-trillion-in-2025), worldwide spending on AI is predicted to total nearly 1.5 trillion in 2025 and exceed 2 trillion in 2026. These figures represent a comprehensive view of the entire AI-enabled economy, including not only direct spending on AI software and services but also the hardware and infrastructure required to support it.Although other estimates tend to be more conservative and based on targeted aspects of the AI market, there is broad agreement among analysts and experts that the AI market is large and growing larger. For example, IDC (https//info.idc.com/futurescape-generative-ai-2025-predictionshtml) forecasts that global enterprises will invest 307 billion on direct AI solutions in 2025 and expects that figure to grow to 632 billion by 2028.

### That'ss a lot of money!

It is And some of that money will be spent by your employers partners customers colleagues and competitors The vast sums spent on AI reflect the degree of potential transformation that AI is bringing to the world.

As of this writing, some analysts and observers have suggested that there is an AI-related financial 'ssbubble'ss (a period of speculative frenzy in which asset prices rise to unsustainable levels far exceeding their fundamental value). Although it'sss impossible to say for sure whether AI-related assets are financially overvalued at this moment, the idea that there may be an 'ssAI bubble'ss is distinct from the concept of AI as a transformational technology with the potential to have a greater impact on society than even the internet or cellular phones And, as with previous bubbles (e.g., the 'ssdotcom bubble'ss, if there is in fact an AI bubble, there will be companies that first survive and then thrive after the downturn passes

---

### What is artificial intelligence (AI)?

### üñºÔ∏è Image: Image Content
**Description:** There are many definitions of AI; scholars and computer scientists still debate over the finer points of what the term means But for our purposes let'sss start with the basics
**üìÅ File Information:**
- **Filename:** stock-image.jpg
- **Location:** scormcontent/assets/Uettbr/stock-image.jpg
- **File Size:** 192.1 KB
- **File Type:** JPG
- **Dimensions:** 1680x943 pixels
- **Access:** Open SCORM zip ‚Üí scormcontent/assets/Uettbr/stock-image.jpg

It might help to think of AI as a 'sssmart'ss computer program that learns from experience and can make decisions like a human would, but usually much faster and with more data. AI is used in things like voice assistants (such as Siri or Alexa), self-driving cars and in retail applications that recommend products similar to those you have purchased in the past. AI tries to mimic human thinking to solve problems and perform tasks that normally require human intelligence. Consider a user who interacts with ChatGPT. The user enters a text prompt and gets a result. But what happens in between? Between the prompt and the result the user receives is matha lot of math. Mass quantities of data are fed into computationally expensive algorithms

### A person without a technical background asks 'ssWhat is AI?'ss

How should you respond? Begin by defining AI in simple terms'ssArtificial intelligence is a branch of computer science that enables machines to perform tasks that typically require human intelligence.'ss

### üñºÔ∏è Image: Image Content
**Alt Text:** What is artificial intelligence? Graphic includes three definitions of artificial intelligence. First, from MIT:
**üìÅ File Information:**
- **Filename:** WhatIsAI_1a.jpg
- **Location:** scormcontent/assets/WhatIsAI_1a.jpg
- **File Size:** 98.4 KB
- **File Type:** JPG
- **Dimensions:** 1680x785 pixels
- **Access:** Open SCORM zip ‚Üí scormcontent/assets/WhatIsAI_1a.jpg

Suppose your conversation partner needs a more robust definition? Consider incorporating the following information into your conversation:Artificial intelligence is a multidisciplinary branch of computer science that simulates human intelligence processes in machines These processes include:Learning (gaining information and rules for using the information)Reasoning (applying rules to arrive at conclusionsSelf-correction AI systems can perform tasks that usually require human intelligence, such as visual perception, speech recognition, decision-making, and language translation. In other words AI describes systems capable of acquiring knowledge and applying insights to enable problem solving.

AI is a vast topic. This course is meant to introduce you to the basics of AI; it will not make you an AI expert. The goal of this course is to enable you to have an informal conversation about AI with colleagues customers and friends

---

### What is machine learning?

### üñºÔ∏è Image: Image Content
**Alt Text:** Machine learning is a subset of AI: ML empowers computers to learn from data and improve their performance with time. Training statistical models to extract knowledge and patterns from data. Training is done using supervised or unsupervised learning ML models can make accurate predictions and decisions
**üìÅ File Information:**
- **Filename:** WhatisML1a.png
- **Location:** scormcontent/assets/WhatisML1a.png
- **File Size:** 51.0 KB
- **File Type:** PNG
- **Dimensions:** 1662x581 pixels
- **Access:** Open SCORM zip ‚Üí scormcontent/assets/WhatisML1a.png

Machine learning (ML) is a subcategory of AI that uses algorithms to identify patterns and make predictions within a set of data. This data can consist of numbers text, or even photos Under ideal conditions ML allows humans to interpret data more quickly and more accurately than they would ever be able to on their own. For instance, ML can be used to anticipate consumer buying patterns based on seasonal factors website traffic, and so forth.

Artificial intelligence (AI) and machine learning (ML) are often linked in conversation. This typically is abbreviated as AI/ML.

---

### What is deep learning?

Deep learning is a specialized form of ML that teaches computers to process data using an algorithm inspired by the human brain. Deep learning teaches computers to learn through observation, imitating the way humans gain knowledge. A deep learning model uses neural networks with three or more computational layersbut typically hundreds or thousands of layersto train the models

Neural networks benefit greatly from graphics processing unit (GPU) hardware. The reason has to do with how, at their core, neural networks work with mathematics through matrix multiplication. This is the same type of math that video game graphics use; hardware originally meant for making video games more realistic has found new value in the AI era.

### üñºÔ∏è Image: Image Content
**Alt Text:** Deep learning is a subset of machine learning. It makes complex correlations between data and learns from examples and previous mistakes -- Uses algorithms to analyze data with a logical structure. -- Uses neural networks (multi-layered structures of algorithms to identify patterns and classify information. Deep learning requires larger amounts of data compared to machine learning.
**üìÅ File Information:**
- **Filename:** WhatIsDeepLearning1a.jpg
- **Location:** scormcontent/assets/WhatIsDeepLearning1a.jpg
- **File Size:** 90.7 KB
- **File Type:** JPG
- **Dimensions:** 1548x654 pixels
- **Access:** Open SCORM zip ‚Üí scormcontent/assets/WhatIsDeepLearning1a.jpg

---

### Takeaways

Here are the takeaways from this lesson:

‚Ä¢ Part of what makes the incredible growth in artificial intelligence (AI) possible is Moore'sss law, which states that the number of transistors in an integrated circuit doubles approximately every two years Although Moore'sss prediction referred specifically to the number of transistors on a microchip, Moore'sss law has also proven to be accurate with regard to the capabilities of hardware in general and other technology.

‚Ä¢ The market for AI-related spending is large and only going to grow larger within the next few years

‚Ä¢ AI is a branch of computer science that enables machines to perform tasks that typically require human intelligence.

‚Ä¢ Machine learning (ML) is a subcategory of AI that uses algorithms to identify patterns and make predictions within a set of data.

‚Ä¢ Deep learning is a specialized form of machine learning that teaches computers to process data by using an algorithm inspired by the human brain and neural networks Deep learning teaches computers to learn through observation, imitating the way humans gain knowledge.

---

### Knowledge check

### üß† Knowledge Check: Which of the following statements best describes Moore'sss law?

**Answer Options:**
A. Any piece of software reflects the organizational structure that produced it.
B. The number of transistors in an integrated circuit doubles approximately every two years ‚úÖ
C. Software gets slower more rapidly than hardware becomes faster.
D. The value of a network is proportional to the square of the number of connected users

**‚úÖ Correct Feedback:** Correct! Moore'ss law states that the number of transistors in an integrated circuit doubles approximately every two years
**‚ùå Incorrect Feedback:** Incorrect. Please try again.

---

### üß† Knowledge Check: Which of the following statements is most accurate?

**Answer Options:**
A. Artificial intelligence (AI) is a subset of deep learning.
B. Artificial intelligence (AI) is a subset of machine learning (ML).
C. Machine learning (ML) is a subset of deep learning.
D. Deep learning is a subset of machine learning (ML). ‚úÖ

**‚úÖ Correct Feedback:** Correct!
**‚ùå Incorrect Feedback:** Incorrect. Please try again.

---


## Predictive AI and Generative AI


### Lesson overview

Even if you'ssve never sought out AI, it'sss likely that you interact with it in your everyday life. This lesson introduces predictive AI and generative AI and explores everyday examples of how individuals and enterprises use both of these types of AI.

### Lesson objectives

‚Ä¢ Define predictive AI and generative AI.

‚Ä¢ Describe everyday examples of predictive AI and generative AI.

‚Ä¢ Explain, at a high level, how organizations might use both types of AI.

---

### What is predictive AI?

### üñºÔ∏è Image: Image Content
**Description:** Predictive AI is one of the most common types of AI used in business applications It has been in use for many years and predicts or forecasts outcomes based on historical data.
**Alt Text:** Predictive AI predicts or forecasts outcomes based on historical data. Predict credit risk or fraud Forecast demand Offer product recommendations More accurately determine patient risk scoring Predict high-risk illnesses
**üìÅ File Information:**
- **Filename:** PredictiveAI_2025.png
- **Location:** scormcontent/assets/PredictiveAI_2025.png
- **File Size:** 28.7 KB
- **File Type:** PNG
- **Dimensions:** 690x782 pixels
- **Access:** Open SCORM zip ‚Üí scormcontent/assets/PredictiveAI_2025.png

Predictive AI that relies on advanced statistical processing methods and uses a different kind of mathematics than neural networks Statistical processing doesn'sst get the same kind of performance boost from GPUs that neural networks do. Neural networks can also be used to find complex patterns needed to make these predictionsA relatively mature technology, predictive AI is widely used in a variety of sectors For example, predictive AI is often used to forecast credit risk or fraud in the financial services industry and to identify which patients are at the most risk for certain illnesses in the medical sector.Let'sss look at some everyday examples of predictive AI.

---

### Predictive AI in your daily life

Whether you realize it or not, you likely interact with predictive AI daily.

To learn more about everyday examples of predictive AI, click either the right-facing arrow or the red Start button below.

### üìã Interactive Process

**Introduction:** Everyday examples of predictive AI
Let'ss take a quick look at some everyday examples of predictive AI.

**Summary:** Continue to the next topic.
Click the Continue bar below to progress to the next topic.

**Step 3:** Text autocompletion: Google 
Every time you use Google to conduct a web search, you interact with AI. When you enter text into Google'sss search box, Google uses AI to offer you autocompletion options for your search as you type. Google'sss AI bases its predictive search completion on the history of previous searches by other users among other data.

**Step 4:** Text autocompletion: Text messaging
Similar to Google searches the autocomplete functionality available with most SMS messaging programs uses AI to predict which word the user is likely to enter next.

**Step 5:** Text autocompletion: Text messaging (cont)
Notice how the iPhone user is offered three options for words that seem likely to follow 'ssCan'sst'ss in this context. The messaging application on the iPhone is using AI to predict which word the user is likely to enter next.

**Step 6:** Online retailer recommendations
When retail websites such as Amazon offer you recommendations of items to purchase, that'sss another example of predictive AI at work.Amazon makes recommendations based on customersss interests and what they have purchased before, and how they have rated their previous purchases In addition, Amazon'sss AI has access to data regarding other customersss purchases and uses that to provide targeted recommendations For example, suppose you are purchasing an iPad on Amazon, and place the iPad in your cart. Amazon then indicates that other customers who purchased iPads also purchased a certain screen protector and a particular type of case. Based on this recommendation, you add the screen protector and case to your cart.

**Step 7:** Movie and music recommendations
Streaming applications such as Netflix and Spotify use predictive AI to make viewing or listening recommendations Suppose you just watched the 1979 film Alien on Netflix, and the application now suggests you watch Mad Max: Fury Road. How does Netflix arrive at this recommendation? Put very simply, data regarding the film you watched (as well as data regarding your viewing preferences and those of other Netflix users including other users who watched Alien) is fed into a model that data scientists have created or trained.

**Step 8:** ‚ÄúData scientists‚Äù? What‚Äôs ‚Äúdata science‚Äù?
Data science is an interdisciplinary field that leverages mathematical, statistical, and computational techniques to extract knowledge and insights from structured and unstructured data. It encompasses various processes from data collection and cleaning to analysis and visualization, ultimately driving decision making in a wide range of domains Data scientists extract knowledge and insight from data and help organizations ask the right questions about what they need AI models to do. Examples of activities data scientists engage in include:Data collection from various sources including databases APIs files and streaming platformsData cleaning to remove errors handle missing values and format the data for analysis Selection of the appropriate AI models or algorithms and training these models using data and evaluate their performance using a variety of metricsEvaluation and validation of the models to confirm that they perform as expectedData scientists use various tools to accomplish their tasks including such things as Jupyter Notebooks PyTorch, and TensorFlow.Data scientists are among the people most likely to be working with Red Hat'sss AI solutions They tend to not be interested in underlying infrastructure concerns in the same way that IT operations personnel are.

---

### Enterprises also use predictive AI.

Just as predictive AI is part of the everyday life of individual users it plays a similar role for enterprises Let'sss explore some examples of how enterprises use predictive AI in their business

### Logistics companies

### üñºÔ∏è Image: Image Content
**Description:** Logistics companies use predictive AI every day to conduct their business and deliver for their customers
**üìÅ File Information:**
- **Filename:** UPSTruck.jpg
- **Location:** scormcontent/assets/UPSTruck.jpg
- **File Size:** 31.2 KB
- **File Type:** JPG
- **Dimensions:** 614x478 pixels
- **Access:** Open SCORM zip ‚Üí scormcontent/assets/UPSTruck.jpg

### Optimizing route planning

Logistics companies such as UPS use predictive AI to optimize route planning, based on data from previous deliveries and forecast the ideal route for delivery trucksDelivery route planning must account for such factors as traffic, fuel consumption, weather conditions and safety considerations This planning can quickly become complex when drivers need to make more than 100 stops each day; however, determining the optimal route can reduce costs increase efficiency, and enhance customer satisfaction.According to its website, UPS delivers 22.3 million packages and documents daily; however, it only has approximately 135,000 trucks and other road vehicles in its fleet. How does UPS deliver so many packages with so few trucks Predictive AI is part of what makes this level of efficiency possible.

By applying predictive AI to package delivery, and accounting for 'sspredicted'ss packages along with known packages UPS annually saves an estimated 18.5 million miles 35 million, 800,000 gallons of fuel, and 18,500 metric tons of emissions

Source: Harvard Business Review (https//hbr.org/2024/01/getting-machine-learning-projectsfrom-idea-to-execution)

### Reducing package theft

### üñºÔ∏è Image: Image Content
**Description:** UPS uses predictive AI to combat package theft.Package theft has increasingly become a problem. According to Capital One Shopping (https//capitaloneshopping.com/research/package-theft-statistics::textNationwide20Package20Theft20StatisticstextOver20352520of20Americans20had,totaled20241.0920billion20in202023.), there were 119 million packages stolen in the U.S. throughout 2023, and Americans lost an estimated 13.4 billion to package theft that year.
**üìÅ File Information:**
- **Filename:** stock-image.jpg
- **Location:** scormcontent/assets/MTPS-b/stock-image.jpg
- **File Size:** 151.7 KB
- **File Type:** JPG
- **Dimensions:** 1680x946 pixels
- **Access:** Open SCORM zip ‚Üí scormcontent/assets/MTPS-b/stock-image.jpg

With predictive AI, UPS is able to predict the likelihood of a successful delivery based on such factors as location, loss frequency, returns volume, and number of delivery attempts The AI groups these factors and can assign a score to a delivery address that reflects the likelihood of successful delivery.

UPS was able to determine that 2 of 'sslow-confidence'ss addresses were involved in 30 of package losses

The technology allows businesses that use UPS and their customers to reroute the most at-risk shipments to a UPS Store or UPS Access Point, giving them alternatives to ensure smooth deliveries and a great customer experience. The solution can also be built into a businesssss ordering platform to improve their delivery outcomes

### Benefits of using predictive AI in logistics

Among the many benefits predictive AI offers logistics companies are the following:Reduced cost, time to deliver, and resource use: By using predictive AI for such things as route optimization, UPS and other logistics companies can reduce cost, deliver more quickly, and reduce fuel consumption and vehicle emissionsPrevention of package theft: By using predictive AI to identify delivery addresses where package theft is most likely to take place, logistics companies can prevent theft by allowing for package rerouting to more secure locations

---

### Banks and financial institutions

### üñºÔ∏è Image: Image Content
**Description:** Banks and other financial institutions routinely use predictive AI for a variety of reasons
**üìÅ File Information:**
- **Filename:** Bank%20vault.jpg
- **Location:** scormcontent/assets/Bank%20vault.jpg
- **File Type:** JPG
- **Dimensions:** 606x657 pixels
- **Access:** Open SCORM zip ‚Üí scormcontent/assets/Bank%20vault.jpg

### Identifying fraud and other financial crimes

### üñºÔ∏è Image: Image Content
**Description:** According to the United Nations (https//www.unodc.org/unodc/en/money-laundering/overview.html::textMoney20laundering20is20a20processthe20trail20to20foil20pursuit)), up to 2 trillion a year is 'sslaundered'ss by criminals attempting to obscure the illegal sources of their money. As one of the world'sss largest banks HSBC uses predictive AI to help it screen an average of more than 1.2 billion transactions per month for evidence of money laundering, fraud, and other financial crime.
**üìÅ File Information:**
- **Filename:** stock-image.jpg
- **Location:** scormcontent/assets/nmmmtB/stock-image.jpg
- **File Size:** 140.6 KB
- **File Type:** JPG
- **Dimensions:** 1680x1120 pixels
- **Access:** Open SCORM zip ‚Üí scormcontent/assets/nmmmtB/stock-image.jpg

Previously, HSBC employed a rulesbased system to review transactions for signs of money laundering, which involved setting parameters for transactions the bank'sss automated monitoring system should identify. Transactions tagged as suspicious were then investigated on a case-by-case basis however, this resulted in a great deal of false positives (i.e., legal transactions that still needed to be investigated by HSBC'sss human investigators.HSBC used its financial expertise to train its AI model to detect suspicious activity with more precision than its previous rulesbased system. This led to the following benefitsMore accurate risk detection: The new AI system reduced the overall number of alerts by 60 while correctly identifying two to four times as much suspicious activity as the previous system.Reduced investigation time: Fewer alerts meant investigators spent less time reviewing clean transactions As a result, HSBC was able to decrease the time it takes from first alert to detect a suspicious account.Better success in identifying criminal networks HSBC'sss new system is better than the previous system at identifying networks of criminals working together to try and launder money.Improved customer satisfaction: Fewer false positives meant the bank spent less time contacting legitimate, law-abiding customers and taking up their time with, ultimately, unnecessary transaction reviews

### Providing AI-enhanced accounts in mobile apps

### üñºÔ∏è Image: Image Content
**Description:** Through its mobile banking app, Wells Fargo uses predictive AI to offer its customers 'sspredictive banking.'ss
**üìÅ File Information:**
- **Filename:** stock-image.jpg
- **Location:** scormcontent/assets/jdWXSw/stock-image.jpg
- **File Size:** 36.2 KB
- **File Type:** JPG
- **Dimensions:** 1680x1132 pixels
- **Access:** Open SCORM zip ‚Üí scormcontent/assets/jdWXSw/stock-image.jpg

With predictive AI, Wells Fargo analyzes customerss account information to provide tailored guidance and tips for better financial decision-making.For example, the feature flags higher-than-normal automatic monthly payments and offers reminders to prompt customers to transfer money from one account to another to avoid a possible upcoming overdraft. If a customer seems to have more money than usual in a particular month, they may be prompted to transfer money into savings

### Benefits of using predictive AI in banking

Among the many benefits predictive AI offers banks are the following:More accurate fraud and money laundering detection: By using predictive AI, banks such as HSBC can identify money laundering with fewer false positives which means human investigators are able to focus on the suspicious accounts most likely to be associated with illegal activity and spend less time reviewing transactions made by legitimate, law-abiding customersTailored guidance and prompts for account holders Banks use predictive AI to help their customers identify unusually high automatic payments and prompt customers to transfer sufficient funds to avoid potential overdrafts

---

### Beyond predictive AI

AI is not limited to merely forecasting likely outcomes making recommendations and so forth; it can do so much more. As AI systems become more sophisticated, they become better at simulating human intelligence, which has exciting implications that are likely to revolutionize almost every industry and convey benefits to your customers

---

### What is generative AI?

### üñºÔ∏è Image: Image Content
**Description:** In contrast to predictive AI, generative AI, or gen AI for short, is AI technology that relies on deep learning models trained on large data sets to create new content. Generative AI tends to be computationally expensive and benefit greatly from GPU acceleration.
**Alt Text:** Generative AI uses larger set of data and models to generate new, original content Conversational Q and A (ChatGPT) Document drafting (for example, blog posts proposals reports summaries Virtual try-ons Image creation (for example, art inspired by a certain film director) Music development (copying existing styles
**üìÅ File Information:**
- **Filename:** GenerativeAI_2025.png
- **Location:** scormcontent/assets/GenerativeAI_2025.png
- **File Size:** 41.3 KB
- **File Type:** PNG
- **Dimensions:** 810x825 pixels
- **Access:** Open SCORM zip ‚Üí scormcontent/assets/GenerativeAI_2025.png

### Everyday examples of generative AI

As with predictive AI, it'sss likely that you'ssve interacted with generative AI in your everyday life. Let'sss go through some real-life examples

### üìã Interactive Process

**Introduction:** Everyday examples of generative AI
Let'ss take a quick look at some everyday examples of generative AI.

**Summary:** Continue to the next topic.
Click the Continue bar below to progress to the next topic.

**Step 3:** Google search: AI Overviews
Just as Google uses predictive AI to offer users autocompletion options in web searches it uses generative AI in its AI Overview feature to offer users AI-generated summaries of search results AI Overview uses advanced ML algorithms to generate these summaries Since October 2024, Google has included inline links within AI Overviews so users can access source content within the AI-generated summaries

**Step 4:** Amazon review summaries
If you'ssve shopped on Amazon recently, you have likely noticed AI-generated summaries of product reviews These summaries can distill hundreds or even thousands of customer reviews of a product down to a single, easy-to-read paragraph that calls out what customers liked or did not like about the product. The AI that reads all of those reviews and writes a summary of them is an example of generative AI.

---

### Other examples of generative AI

Let'ss take a look at some other examples of generative AI.

### Chatbots

### üñºÔ∏è Image: Image Content
**Description:** Chatbots such as ChatGPT, are examples of generative AI.
**üìÅ File Information:**
- **Filename:** ChatGPT_logo.png
- **Location:** scormcontent/assets/ChatGPT_logo.png
- **File Size:** 6.3 KB
- **File Type:** PNG
- **Dimensions:** 512x512 pixels
- **Access:** Open SCORM zip ‚Üí scormcontent/assets/ChatGPT_logo.png

A chatbot is a computer program designed to simulate conversation with human users especially over the internet. It uses AI techniques to understand language and respond accordingly. Chatbots can be found in messaging applications websites and other platforms where they help users with tasks like answering questions providing information, or even engaging in casual conversation. Chatbots such as ChatGPT, are examples of generative AI because they can generate new content. For example, a user might write a prompt asking ChatGPT to write a sonnet, and ChatGPT would write the poem. Likewise, a user could write a prompt asking ChatGPT to write code to perform a certain task, and ChatGPT would generate the code.

### Benefits of chatbots

Chatbots offer users the following benefitsFast generation of text-based content: With chatbots such as ChatGPT, users can quickly generate all manner of text-based content about practically any subject one could imagine, and do so much faster than a human could. Code generation: In addition to writing in standard prose, chatbots are often capable of writing code for a variety of usesTranslation: Many chatbots can also translate text from one language to another, again much faster than a human could.

---

### Enhancing sporting events

### üñºÔ∏è Image: Image Content
**Description:** At the Wimbledon Tennis Championships IBM uses generative AI to enhance the fan experience for online, application, and TV users and as well as assess player performance.
**üìÅ File Information:**
- **Filename:** stock-image.jpg
- **Location:** scormcontent/assets/KfrlT7/stock-image.jpg
- **File Size:** 62.2 KB
- **File Type:** JPG
- **Dimensions:** 1680x880 pixels
- **Access:** Open SCORM zip ‚Üí scormcontent/assets/KfrlT7/stock-image.jpg

IBM uses generative AI in the form of its Watson X platform and a Wimbledon-specific AI model to process more than 2.5 million data points during the tournament to deliver real-time data insights which are fed to producers and commentators providing the TV coverage, enhancing the viewing experience. The model can generate tennisspecific content such as player profiles which are shared with fans to improve their experience. IBM also uses its AI to create highlight reels as well as the commentary for those highlights which also serves to enhance the fan experience. The AI is capable of grasping nuances such as drop shots and other player tactics which provides players and their coaches with valuable insights IBM'sss generative AI can also assess playerss chances of winning and even provide suggestions for refining their play strategies

### Benefits of using AI to enhance experiences of sporting events

Using AI to enhance usersss experiences of sporting events provides the following benefitsReal-time data insights Data insights created by generative AI are sent almost instantly to broadcasters and commentators so those insights can be shared with the viewing audience.AI-generated content: Generative AI can create such things as player profiles that can enhance the audience'sss experience of the event. AI can also generate highlight reels and accompanying commentary for those fans unable to watch the live event.

---

### Now, pause for a knowledge check.

Do you know the difference between predictive AI and generative AI? Drag each card to the type of AI to which it belongs

### üóÇÔ∏è Sorting Activity: Interactive Sorting

**Categories:**
‚Ä¢ Predictive AI
‚Ä¢ Generative AI

**Items to Sort:**

**Predictive AI:**
‚Ä¢ Autocomplete in a texting app
‚Ä¢ Netflix making a movie recommendation
‚Ä¢ A logistics company identifying the optimal route for its delivery trucks
‚Ä¢ An online retailer making a product recommendation based on items in your cart
‚Ä¢ Identifying the banking transactions most likely to involve fraud

**Generative AI:**
‚Ä¢ A chatbot writing sonnet based on a text prompt
‚Ä¢ Amazon summarizing customers' reviews of a product
‚Ä¢ A chatbot writing code
‚Ä¢ AI creating a highlight reel from a sporting event 
‚Ä¢ AI generating player profiles during a tennis tournament

---

### What is a large language model (LLM)?

A large language model (LLM) is a type of AI program designed to understand and generate human language. It'sss referred to as 'sslarge'ss because it'sss built on huge amounts of text data, which it uses to learn patterns and relationships in language. LLMs can answer questions generate text, translate languages and perform other language-related tasks with a high level of accuracy and fluency.

Since November 2022, when ChatGPT launched, people have tended to conflate LLMs with AI in general. In reality, AI is the broader topic and LLMs are advanced tools within the larger AI ecosystem.

It takes an incredible amount of resources and expertise to create an LLM; the vast majority of organizations are not capable of building their own LLM. LLMs are what make chatbots such as ChatGPT possible, and they enable the seemingly limitless potential that AI has to offer.

### Using models with generative AI

Very few organizations have the resources data, and expertise necessary to build their own LLMs from scratch (this is sometimes referred to as model training). Organizations that are interested in generative AI will most likely start by using one of several foundation models however, those models require additional work before they can be used effectively. There are a variety of approaches for accomplishing this

A foundation model is a type of ML model pretrained to perform a range of tasks Foundation models are programmed to function with a general contextual understanding of patterns structures and representations This foundational comprehension of how to communicate and identify patterns creates a baseline of knowledge that can be modified, or fine-tuned, to perform domain-specific tasks for just about any industry. Examples of foundation models include GPT, Claude, and Llama.

### üñºÔ∏è Image: Image Content
**Alt Text:** The balancing act of using foundation models Foundation models still need more work to be useful Prompt tuning allows data scientists to adapt models offering 'sgood enough's accuracy but doing it with less resources Retrieval-augmented generation (RAG) is a method for getting better answers from a generative AI application by linking an LLM to an external resource Fine tuning foundation models requires a high amount of resources (data, hardware, people) Training a foundation model from scratch requires an unrealistic amount of computing, and goes against the principles of reusing foundation models
**üìÅ File Information:**
- **Filename:** TheBalancingActOfUsingFoundationModels.jpg
- **Location:** scormcontent/assets/TheBalancingActOfUsingFoundationModels.jpg
- **File Size:** 102.3 KB
- **File Type:** JPG
- **Dimensions:** 1680x880 pixels
- **Access:** Open SCORM zip ‚Üí scormcontent/assets/TheBalancingActOfUsingFoundationModels.jpg

Click the plus () signs in the following interactive exercise to read more about models and different approaches to using them.

### Interactive Accordion

**Tokens**
Tokens are small units of textsuch as words subwords or characters that have semantic meaning for a model. To process language, LLMs break down text into tokens These tokens are how the model 'ssreadsss and 'sswritesss language, helping it understand meaning, context, and grammar. For a general sense of the word-to-context ratio, a sentence containing 10 words might be 15 to 20 tokensEvery model has a limit to the number of tokens that can be included in the input prompt as well as the number of tokens that appear in the generated output from the model. This limit is sometimes called context window length or context window.In many ways tokens represent the currency of AI, and token cost is likely to be a consideration for enterprise organizations using AI.

**Model training**
Model training refers to the initial phase of building the AI/ML model in which the model learns from a large dataset to understand patterns relationships and features in the data. Creating AI foundation models from scratch can be very resource- and time-intensive and is in reach of only a few enterprise customers

**Fine-tuning**
Fine-tuning is a technique that involves taking a pretrained generative AI model and further training it on a specific dataset or for a specific task. Fine-tuning requires a labeled dataset that is specific to the task to train the model with examples of input-output pairs related to that task. Although fine-tuning requires significantly less data than the original training process it still generally involves a large investment of resources and vast amounts of data. In general, fine-tuning is more involved and labor intensive than prompt-tuning.

**Retrieval-augmented generation (RAG)**
Retrieval-augmented generation (RAG) is a method for getting better answers from a generative AI application by linking an LLM to an external resource. RAG provides a means to supplement the data that exists within an LLM with external knowledge sourcesfor example, data repositories collections of text, and pre-existing documentation. These resources are segmented, indexed in a vector database, and used as reference material to deliver more accurate answers RAG is useful because it directs the LLM to retrieve specific, real-time information from a chosen source (or sources of truth. RAG can save money by providing a custom experience without the expense of model training and fine-tuning. It can also save resources by sending only the most relevant information (rather than lengthy documents when querying an LLM. As of this writing, RAG is one of the most popular approaches to working with foundation models

**Prompt engineering**
Prompt engineering refers to the process of carefully crafting prompts entered into LLMs to generate accurate, high-quality outputs It does not require coding skills it requires clear communication, logic, and an understanding of how the AI worksEffective prompting starts with structure. There is no single right way to do it, but successful approaches share a consistent design logic: they make intent explicit and repeatable. One useful structure frames each interaction around these four elements Context: Supply the background and objectives Role: Assign the model a perspective or domain identity. Clarification: Encourage the model to ask questions before acting. Task: Assign a specific job to perform. This is only one of many proven methods Several other prompt engineering frameworks have emerged across the industry, each emphasizing a different aspect of clarity and control. These frameworks share the same intent: to transform prompting from ad hoc experimentation into a disciplined design practice.Prompt engineering requires less technical knowledge than prompt-tuning.

**Prompt-tuning**
Taking the discipline of prompt engineering to a new level, prompt-tuning is a technique data scientists use to optimize the prompts or instructions a user gives to an AI model to experiment with different prompt formats and wording to achieve the desired results In some cases prompt-tuning allows organizations to adapt models and achieve 'ssgood enough'ss accuracy but to do it with less resources Prompt-tuning requires more technical knowledge than prompt engineering; it involves using a small, specialized dataset to train a set of numeric vectors (often referred to as 'ssoft promptss that are prepended to the input. Unlike prompt engineering, prompt-tuning requires data science expertise.Prompt-tuning is often contrasted with fine-tuning AI models which tends to require more effort and resources

**Model inferencing**
Model inferencing is the phase in which the trained AI/ML model is put to use and can make predictions generate text, classify data, or perform any other task it was designed for. During inference, the trained model takes in new, unseen data and produces outputs based on its learned patterns

The following lesson covers model inferencing in more depth.

---

### Potential issues with LLMs

As incredibly powerful as LLMs are, there are issues associated with them that you should be aware of as you discuss AI.

### Massive energy, water, and resource costs

Building an LLM from scratch is prohibitively expensive for most organizations Of course, there'sss the hardware that'sss required, along with data centers and other expensesand that'sss before you factor in the cost of training the LLM.The energy cost alone associated with training and maintaining an LLM is staggering, both from financial and sustainability perspectives For example, training GPT-3 (with its 175 billion parameters consumed an estimated 1,287 megawatt hours of electricity, which is roughly equivalent to the energy consumption of an average American household over 120 years

### üñºÔ∏è Image: Image Content
**Alt Text:** Photo of an LED lightbulb Text: 'sA single LLM interaction may consume as much power as leaving a low-brightness LED lightbulb on for one hour.'s - Alex de Vries VU Amsterdam
**üìÅ File Information:**
- **Filename:** LEDbulb1.jpg
- **Location:** scormcontent/assets/LEDbulb1.jpg
- **File Size:** 24.2 KB
- **File Type:** JPG
- **Dimensions:** 1205x349 pixels
- **Access:** Open SCORM zip ‚Üí scormcontent/assets/LEDbulb1.jpg

In addition to the electricity demands of LLMs such models also require a tremendous amount of water to cool the hardware and data centers used by LLMs According to scientists from the University of California, Riverside (https//arxiv.org/pdf/2304.03271), the global AI demand for water is projected to account for 4.2 to 6.6 billion cubic meters of water withdrawal in 2027, which is more than half of the annual water withdrawal of the entire United Kingdom. Even the cost of maintaining an LLM is exceptional. Consider this Back in 2023, according to research firm SemiAnalysis (https//newsletter.semianalysiscom/p/the-inference-cost-of-search-disruption), it cost OpenAI approximately 700,000 a day in compute hardware costs to operate ChatGPT. That'sss about 255 million a year; however, the current cost of operating ChatGPT is likely much greater. An estimate included in a TechCrunch (http://echcrunch.com/2025/11/14/leaked-documentsshed-light-into-how-much-openai-paysmicrosoft/::textWAITLIST20NOW,the20first20half20of202025.) story had OpenAI spending roughly 3.8 billion on inference in 2024 and an estimated 8.65 billion on inference in the first nine months of 2025.

### Issues with transparency in model building

In most instances there is no way to know for sure which sources were used to create and train a particular LLM. As the user of the LLM, you have no way of knowing for certain whether the content used to train the model was accurate, which could lead to flawed, incomplete, or erroneous results

### Bias and model building

Unfortunately, it is impossible to prevent biases from leaking into models which can affect output. For example, minorities may be underrepresented in the data used to train the model, which may lead to skewed outcomes A 2023 study found that four recently published LLMs were three to six times more likely to choose an occupation that stereotypically aligns with a person'sss gender than what official occupation data would suggest. And despite the best intentions of the model creators there is a risk of overcorrection for bias (such as racial bias, that leads to historically inaccurate results

### Hallucinations

Sometimes LLMs can generate responses that appear coherent, grammatically correct, and are stated in a manner that suggests they are accurate, when in fact, they are nonsensical or factually incorrect. Such responses are generally referred to as hallucinations For example, consider how ChatGPT 3.5 responded to the following prompt in July 2024: How many 'ssm'sss are there in 'ssWeather'ss

### üñºÔ∏è Image: Image Content
**üìÅ File Information:**
- **Filename:** ChatGPT_3_5.jpg
- **Location:** scormcontent/assets/ChatGPT_3_5.jpg
- **File Size:** 13.5 KB
- **File Type:** JPG
- **Dimensions:** 1680x344 pixels
- **Access:** Open SCORM zip ‚Üí scormcontent/assets/ChatGPT_3_5.jpg

ChatGPT 3.5 claimed there was one 'sm's in 'sWeather.'s Let'ss see how the chatbot did with a second chance.

### üñºÔ∏è Image: Image Content
**üìÅ File Information:**
- **Filename:** ChatGPT_3_5b.jpg
- **Location:** scormcontent/assets/ChatGPT_3_5b.jpg
- **File Size:** 21.1 KB
- **File Type:** JPG
- **Dimensions:** 1680x523 pixels
- **Access:** Open SCORM zip ‚Üí scormcontent/assets/ChatGPT_3_5b.jpg

Later versions of ChatGPT are more likely to answer this question correctly. On a similar note, some older versions of LLMs had issues with counting the number of R'sss in 'ssStrawberry.'ss This flaw is a perfect demonstration of tokenization. LLMs don'st process text character-by-character; they use larger chunks called tokens Instead of seeing ten individual letters (S-t-r-a-w-b-e-r-r-y), the model might see the word broken into just two tokens such as 'sStraw's and 'sberry's. Because the model treats 'sberry's as a single, indivisible unit of meaning, it processes the entire block without breaking it down into its constituent letters The three 'sR'sss are therefore hidden inside these larger tokens which made it difficultor in some cases impossiblefor the model to perform the character-level counting task accurately.

Content generated by AI should be reviewed by humans to confirm its accuracy.

### Issues related to data recency

The information that LLMs have been trained on doesn'sst continuously gather updates there'sss an effective 'sscutoff'ss date to an LLM'sss knowledge. As a result, that source material can become outdated and no longer relevant.Data scientists and researchers may use RAG to address such issues For example, implementing RAG architecture into an LLM-based question-answering system provides a line of communication between an LLM and the chosen additional knowledge sources The LLM is able to crossreference and supplement its internal knowledge, providing a more reliable and accurate output for the user making a query.

### Issues related to data sovereignty

Data sovereignty refers to the principle that data (such as personal information, financial records or intellectual property) is subject to the laws and regulations of the country or region (such as the European Union) where it is collected, processed, or stored. Many nations have laws related to data sovereignty. When it comes to LLMs and data sovereignty, it matters a great deal who owns and operates that LLM, where that LLM'ss servers are located (which may not always be obvious to the end user of the LLM), and what data may have been used to train that model.Consider the following example. A user in Country A enters a prompt containing sensitive data into an LLM, and that LLM has its servers in Country B. The user in Country A has likely lost control of that data, which now could be stored in Country B, used to train future models or inadvertently exposed to other users of the LLMall in violation of Country A'ss data privacy laws

Red Hat'ss open hybrid cloud portfolio, including Red Hat AI, provides sovereignty solutions to address such issues

### Risks related to copyright and privacy

There may be copyright or other legal issues associated with the content used to train the LLM.For example, suppose that an LLM was trained on copyrighted material without the permission of the respective copyright holders If your organization were to use that LLM, and you inadvertently violated the rights of copyright holders your organization could be at risk of being sued.

This legal issue is not merely theoretical; in December 2023, the New York Times sued OpenAI and Microsoft for copyright infringement, contending that millions of articles published by the newspaper were used to train chatbots As of this writing, the case is pending.In December 2025, the New York Times sued Perplexity AI, claiming that the artificial intelligence startup repeatedly violated the newspaper'ss copyrights by copying, distributing, and displaying Times articles without permission.

There are also privacy issues associated with LLMs There is no way to get an LLM to 'ssforget'ss information it'sss been trained on. It'sss not like a spreadsheet, where you could delete a column that contained private information such as dates of birth, Social Security numbers or sensitive medical information. Also, as it becomes easier to use AI to create so-called 'ssdeepfakesss (i.e., highly convincing fake or manipulated information, such as photos videos or audio recordings, the opportunities to use AI to engage in unethical or illegal behavior increase.

### Risks of mass adoption

Currently, there are multiple LLMs in use; however, if one model overwhelmingly beats the competition, and that LLM is effectively, the only LLM, it could change society'sss common understanding of history based on quirks in how the model was trained.

Red Hat believes that the future of AI involves open source practices and transparency in datasets

---

### Takeaways

Here are the takeaways from this lesson:

‚Ä¢ Predictive AI is a common type of artificial intelligence system used in business applications that predicts or forecasts outcomes based on historical data.

‚Ä¢ Predictive AI is an integral part of many everyday activities such as conducting web searches texting, shopping online, and engaging with video and music streaming services

‚Ä¢ Data science is an interdisciplinary field that leverages mathematical, statistical, and computational techniques to extract knowledge and insights from structured and unstructured data.

‚Ä¢ Examples of enterprises using predictive AI include logistic companies employing it to optimize delivery routes and prevent package theft and banks or other financial institutions using it to identify fraud, money laundering, and other financial crimes

‚Ä¢ Generative AI is AI technology that relies on deep learning models trained on large data sets to create new content.

‚Ä¢ Some examples of generative AI include AI-generated summaries of customer reviews on Amazon, chatbots such as ChatGPT generating content based on a text prompt, and AI-generated highlight reels from sporting events

‚Ä¢ A large language model (LLM) is a type of AI program designed to understand and generate human language.

‚Ä¢ Creating an LLM from scratch requires a tremendous amount of money, expertise, and resources For all but a handful of organizations creating such a model is out of reach; most organizations are likely to start with a foundation model.

‚Ä¢ There are a variety of approaches an organization might use when working with a foundation model. One of the most popular approaches is retrieval-augmented generation (RAG), a method that involves getting better answers from a generative AI application by linking an LLM to an external resource.

‚Ä¢ There are several issues associated with LLMs including massive energy and resource costs a lack of transparency with regard to how models are trained, biases in model training, hallucinations data recency, data sovereignty issues and issues related to copyright and privacy.

---

### Knowledge check

### üß† Knowledge Check: Which of the following statements does NOT describe an example of predictive AI?

**Answer Options:**
A. An iPhone user takes advantage of the autocomplete feature when texting to compose a text message more quickly.
B. Amazon uses AI to generate text summaries of hundreds or thousands of customer reviews so users do not need to read all the product reviews for an individual product. ‚úÖ
C. Netflix uses AI to make recommendations to users based on their viewing history and other data.
D. UPS uses AI to determine which delivery addresses are at the highest risk of package theft so it can reroute packages to a more secure location.

**‚úÖ Correct Feedback:** Correct! Although Amazon does use AI to generate summaries of customer reviews that is an example of generative AI, not predictive AI.
**‚ùå Incorrect Feedback:** Incorrect. Please try again.

---

### üß† Knowledge Check: Which statement regarding predictive AI is most accurate?

**Answer Options:**
A. A good example of predictive AI is ChatGPT.
B. It is used to generate text, illustrations and music.
C. It is a relatively new technology.
D. It can be used in the financial industry to identify money laundering. ‚úÖ

**‚úÖ Correct Feedback:** Correct! Banks use predictive AI to identify money laundering.
**‚ùå Incorrect Feedback:** Incorrect. Please try again.

---

### üß† Knowledge Check: Which of the following tools is most closely associated with the work data scientists perform?

**Answer Options:**
A. Jupyter Notebooks ‚úÖ
B. Android Studio
C. Bitbucket
D. Visual Studio Code

**‚úÖ Correct Feedback:** Correct! Among the choices presented, Jupyter Notebooks are the most closely associated with the work that data scientists typically perform.
**‚ùå Incorrect Feedback:** Incorrect. Please try again.

---

### üß† Knowledge Check: True or false: Generative AI involves the use of deep learning models

**Answer Options:**
A. True ‚úÖ
B. False

**‚úÖ Correct Feedback:** True. Generative AI involves the use of deep learning models

---

### üß† Knowledge Check: Which of the following statements regarding large language models (LLMs is most accurate?

**Answer Options:**
A. They require a tremendous amount of resources and expertise to create and train. ‚úÖ
B. They typically provide full transparency regarding how they are trained and developed.
C. It is easy to delete information from LLMs
D. Most LLMs are not trained on copyrighted material.

**‚úÖ Correct Feedback:** Correct! LLMs require a tremendous amount of resources and expertise to create and train. Creating an LLM from scratch is beyond the reach of most organizations
**‚ùå Incorrect Feedback:** Incorrect. Please try again.

---


## Inferencing and Optimization


### Lesson overview

This lesson offers a high-level look at inferencing and its importance to artificial intelligence (AI). It also covers AI optimization, including a brief view of some of the more common optimization techniques

### Lesson objectives

‚Ä¢ Define inferencing in the context of AI.

‚Ä¢ Describe the importance of inference servers to AI.

‚Ä¢ Explain, at a high level, the trade-offs involved in inferencing.

‚Ä¢ Describe the concept of distributed inference.

‚Ä¢ Explain the importance of AI optimization.

‚Ä¢ Describe, in broad terms AI optimization strategies

---

### What is inferencing?

Inferencing, in the context of AI, is the process of running an AI model to generate responses an AI model provides an answer based on data. What some people generally call 'ssAI'ss is really the success of AI inference: the final stepthe 'ssaha'ss momentin a long and complex process of ML technology. AI inference is where the business value happens and the end result is delivered.Training AI models with sufficient data can help improve AI inference accuracy and speed.

### üñºÔ∏è Image: Image Content
**Alt Text:** Image title 'sWhat is inferencing?'s On the left side of image there is an illustration of a person sitting in front of laptop, labelled 'sUser using AI app's on the right side, there is an image of a robot'ss face labelled, 'sAI model processing answer with an inference server's, There is an arrow going from the user to the AI model (left to right) labelled, 'sUser submits request's There is an arrow going from the AI model to the user (right to left) labelled, 'sAI returns a response's At the bottom right of image, there is the following text 'sGoal: Meet user expectations Return the most accurate response as fast as possible.'s
**üìÅ File Information:**
- **Filename:** WhatIsInferencing.png
- **Location:** scormcontent/assets/WhatIsInferencing.png
- **File Size:** 38.7 KB
- **File Type:** PNG
- **Dimensions:** 1212x650 pixels
- **Access:** Open SCORM zip ‚Üí scormcontent/assets/WhatIsInferencing.png

---

### What is an inference server?

An inference server is the crucial piece of software infrastructure that hosts a trained ML model, making the AI ready for real-time use by end-users or other applications The server receives an input request (a prompt, a query, or data), feeds it through the trained model, and generates a real-time output or conclusion based on the model'ss learned patternsthis applies equally to predictive AI (like scoring a recommendation) and generative AI (like creating text or images. While inference servers are necessary for any ML model serving, the current industry focus is heavily centered around the high-demand requirements for LLM serving, which require significant memory and computing resources to perform inference at scale.To infer is to conclude based on evidence. For example, you may see your friend'sss living room light on, but you don'sst see them. You may infer that your friend is home, but you don'sst have absolute evidence to prove it.An LLM also doesn'sst have absolute evidence about the meaning of a word or phrase (it'sss a piece of software), so it uses its training as evidence. In a series of calculations based on data, it generates a conclusion. Just like when you calculate that if the light is off, it means your friend is out.To perform effectively, LLMs need extensive storage, memory, and infrastructure to inference at scale.

---

### What are the trade-offs regarding inferencing?

When moving from a proof-of-concept to production, the demands of AI solutions change significantly. It'ss not just about a clever model anymore. Organizations need to pay attention to user expectations the budget, and the level of responsiveness an IT system should have. Optimizing for one thing often impacts another; optimal inference requires identifying the trade-offsUsers expect a lot of their interaction with models that interaction needs to be fast and provide accurate responsesModel inference can be expensive if processing times and length of model responses aren'sst measured and controlled.All areas of the organization expect business operations won'sst be affected by low throughput and high latency.

### üñºÔ∏è Image: Image Content
**Alt Text:** Image title Requirements of enterprise AI production systems Image subtitle Identifying the trade-offs of inference, Three items of text, First item Need to be fast and accurate in their responses Second item Manage processing times and token output to control const, Third item Deliver high throughput and lower latency for best performance
**üìÅ File Information:**
- **Filename:** ReqsOfEnterpriseAIProductionSystems1.jpg
- **Location:** scormcontent/assets/ReqsOfEnterpriseAIProductionSystems1.jpg
- **File Size:** 51.0 KB
- **File Type:** JPG
- **Dimensions:** 1680x666 pixels
- **Access:** Open SCORM zip ‚Üí scormcontent/assets/ReqsOfEnterpriseAIProductionSystems1.jpg

---

### Scaling LLM inference

Operationalizing AI models at scale is a critical challenge for IT leaders Effectively serving LLMs at scale requires a strategic, full-stack approach that addresses both the model itself and the serving runtime. A single approach is not enough. Achieving high performance and cost efficiency requires a dual focusmanaging resource consumption and maximizing throughput.

---

### What is vLLM?

### üñºÔ∏è Image: Image Content
**Description:** One approach to achieving inference at scale involves using vLLM.
**Alt Text:** VLLM logo
**üìÅ File Information:**
- **Filename:** vLLM_Logo.png
- **Location:** scormcontent/assets/vLLM_Logo.png
- **File Size:** 3.8 KB
- **File Type:** PNG
- **Dimensions:** 498x181 pixels
- **Access:** Open SCORM zip ‚Üí scormcontent/assets/vLLM_Logo.png

An open source library and serving engine, vLLM (https//github.com/vllm-project/vllm) is designed to significantly accelerate the inference (generation of outputs of LLMs It was originally developed at UC Berkeley and has become a popular solution for deploying LLMs efficiently and cost-effectively in production environments to the extent that many in the AI community consider it the de facto standard for model inferencing.

### üñºÔ∏è Image: Image Content
**Alt Text:** Title The value of vLLM Subtitle Deliver fast, flexible, and scalable inference. Main point 1 Faster response time: vLLM can achieve higher throughput. This translates to processing more tasks or requests within a given amount of time. Main point 2 Reduced hardware costs vLLM offers a more efficient use of resources which is equivalent to fewer GPUs needed to handle the processing of LLMs Main point 3 Efficient memory management: vLLM organizes virtual memory. This translates to handling larger models and longer sequences more effectively within a given hardware setup. Main point 4 Designed for security and scale: Self-hosting an LLM with vLLM provides more control over data privacy and usage, as well as an ability to handle growing demand.
**üìÅ File Information:**
- **Filename:** ValueOfVLLM.png
- **Location:** scormcontent/assets/ValueOfVLLM.png
- **File Size:** 52.4 KB
- **File Type:** PNG
- **Dimensions:** 1382x738 pixels
- **Access:** Open SCORM zip ‚Üí scormcontent/assets/ValueOfVLLM.png

---

### What is distributed inference?

Distributed inference lets AI models process workloads more efficiently by dividing the labor of inference across a group of interconnected devices Think of it as the software equivalent of the saying, 'ssmany hands make light work.'ss Distributed inference supports a system that splits requests across a fleet of hardware, which can include physical and cloud servers From there, each inference server processes its assigned portion in parallel to create an output. The result is a resilient and observable system for delivering consistent and scalable AI-powered services

### How does distributed inference work?

Distributed inference works by giving AI models a single, intelligent coordinator that acts as the brain for AI workloads When a new request comes in, distributed inference helps analyze the request and route it to the best part of the hardware system for the job.Large models too many users or latency issues can all hinder performance. Depending on the issue causing the bottleneck, distributed inference uses several strategiesDividing the model: If the model is too large for one GPU, distributed inference uses model parallelism to divide the model across multiple GPUsDividing the data: To handle many users at once, it uses data parallelism and intelligent load balancing to divide the input data across servers This efficiently manages concurrent requestsDividing the inference process To optimize the entire workflow, it uses process disaggregation. This separates the two computational phases that create an inference response (prefill and decode) and runs them in separate environments

---

### What is optimization in the context of AI?

### üñºÔ∏è Image: Image Content
**Description:** We hear a lot about groundbreaking AI innovations incredible new models and the amazing things that they can do. But there'ss a quieter, often overlooked challenge that'ss crucial for actually making AI work in the real world, and that'ss optimization. So what exactly is optimization in the context of AI? It'ss not just about saving money.
**üìÅ File Information:**
- **Filename:** ImageOptimization.jpg
- **Location:** scormcontent/assets/ImageOptimization.jpg
- **File Size:** 110.3 KB
- **File Type:** JPG
- **Dimensions:** 1024x1024 pixels
- **Access:** Open SCORM zip ‚Üí scormcontent/assets/ImageOptimization.jpg

Optimization is about unlocking performance, reliability, and reach. An optimized model delivers faster predictions uses less memory, and can run on more diverse hardware. It extends the life of existing infrastructure and allows AI to serve more users with fewer resources

Think of it this way: Innovation is like designing a sleek, powerful sports car. It'ss exciting. It'ss groundbreaking. But optimization, that'ss making sure that your car can actually drive on everyday roads reliably and affordably without guzzling all your fuel. Remember: If AI can'st scale, it can'st succeed.Training an AI model gets all the headlines It'ss where the model learns But inference is the day-to-day act of putting that trained model to work, making predictions powering applications in real time. And this is where cost, latency, and user experience really collide.Too often, AI systems are built with massive oversized models chosen purely for their benchmark performance. It'ss like using a sledgehammer to tighten a loose screw. The result, excessive GPU usage, frustrating latency, and infrastructure costs that quietly add up. And with the rise of huge foundation models like Llama, Mistral, and DeepSeek, the computational cost of inference has gone through the roof. Optimization isn'st just an option anymore; it'ss essential to make these models usable at scale.

Optimization makes AI powerful and practical.

---

### What are some key optimization techniques

Let'ss take a look at some key optimization techniques

### Interactive Accordion

**Quantization**
This reduces the numerical precision of the model, making it smaller and faster. For example, Red Hat'ss evaluations on Llama models showed a 3.5 times size reduction and a 2.4 time speed up with almost no loss in accuracy.

**Batching**
This groups multiple inference requests together, improving throughput without changing the model itself.

**Caching**
This eliminates redundant computations especially useful for repeated or similar queries

**Pruning and distillation**
These techniques reduce model complexity by removing unnecessary parts or training smaller models to mimic the performance of larger ones

Every optimization involves trade-offs In critical applications such as those related to healthcare or finance, even a tiny shift in accuracy can have major consequences Optimization techniques should be carefully evaluated to properly balance precision with performance goals

---

### Takeaways

Here are the takeaways from this lesson:

‚Ä¢ Inferencing, in the context of AI, is the process of running an AI model to generate responses an AI model provides an answer based on data.

‚Ä¢ An inference server is the piece of software that allows AI applications to communicate with LLMs and generate a response based on data. Inference servers feed the input requests through an ML model and return an output.

‚Ä¢ Distributed inference lets AI models process workloads more efficiently by dividing the labor of inference across a group of interconnected devices

‚Ä¢ In the context of AI, optimization is about unlocking performance, reliability, and reach.

‚Ä¢ Key AI optimization techniques include quantization, batching, caching, and pruning and distillation.

---

### Knowledge check

### üß† Knowledge Check: Which of the following statements most accurately describes an inference server?

**Answer Options:**
A. It is a specialized type of large language model (LLM) exclusively used with agentic AI workflows
B. It is a piece of software that allows AI applications to communicate with large language models (LLMs and generate a response based on data. ‚úÖ
C. It is the safest place to store model training data to ensure data sovereignty.
D. It is a dedicated security appliance that performs threat deduction.

**‚úÖ Correct Feedback:** Correct! An inference server is a piece of software that allows AI applications to communicate with large language models (LLMs and generate a response based on data.
**‚ùå Incorrect Feedback:** Incorrect. Please try again.

---

### üß† Knowledge Check: Which of the following is an AI optimization technique?

**Answer Options:**
A. Containerization
B. Monte Carlo
C. Quantization ‚úÖ
D. Conversion rate optimization (CRO)

**‚úÖ Correct Feedback:** Correct! Quantization is an AI optimization technique. Other examples of AI optimization techniques include batching, caching, and pruning and distillation.
**‚ùå Incorrect Feedback:** Incorrect. Please try again.

---


## Agentic AI


### Lesson overview

In a previous lesson, you learned about predictive AI and generative AI. This lesson concerns agentic AI, a comparatively new form of AI in which some of the most compelling and transforming aspects of AI are being realized.

### Lesson objectives

‚Ä¢ Define agentic AI.

‚Ä¢ Describe examples of agentic AI.

‚Ä¢ Explain the difference between agentic AI and generative AI.

‚Ä¢ Describe, at a high level, how Model Context Protocol (MCP) works and how it relates to agentic AI.

---

### What is agentic AI?

Agentic AI is an AI system designed to interact with data and tools in a way that requires minimal human intervention. With an emphasis on goal-oriented behavior, agentic AI can accomplish tasks by creating a list of steps and performing them autonomously.Unlike traditional AI models which operate within predefined constraints and require a greater degree of human intervention, agentic AI exhibits autonomy, goal-driven behavior, and adaptability. The term agentic refers to these modelsss agency, or, their capacity to act independently and purposefully.

While AI has already redefined how we interact with data, automate tasks and serve customers the next phase in AI is emerging, pushing beyond content generation and predictive analytics

Think of agentic AI as a way of combining automation with the creative abilities of an LLM. To bring agentic AI to practice, you create a system that provides an LLM with access to external tools and algorithms that supply instructions for how the AI agents should use those toolsThe way agents communicate with tools involves orchestration, with flows or graphs depending on the framework being used. This approach allows the LLM to 'ssreason'ss and determine the best way to answer a questionsuch as deciding whether the query can be answered with available information or whether an external search is necessary.

---

### What is an AI agent?

Think of an AI agent as an entity that sits on top of other software tools and operates them. Agentic AI can be a physical structure, a software program, or a combination of the two.

### üñºÔ∏è Image: Image Content
**Alt Text:** Title AI agents integrate models functions and tools Subtitle Gen AI models predictive AI models code functions search, and more Diagram, left to right, depicts a User, an AI agent, and three vertically stacked items 1 LLM, 2 Code executor, and 3 Data, document, or web Under User, there is the following text 'sComplex task'ss under AI agent, there is the following text 'sNLP to understand prompt, generate task list, and execute's There is an arrow labelled 'sPrompt's going from User to AI agent, and an arrow labelled 'sResponse's going from AI agent to User. From AI agent to 1 LLM there is an arrow labelled 'sNLP Query's and from 1 LLM to AI agent, there is an arrow labelled 'sModel response's From AI agent to 2 Code executor, there is an arrow labelled 'sCode's and from 2 Code executor to AI agent, there is an arrow labelled 'sExecution response's From AI agent to 3 Data, document, or web, there'ss an arrow labelled 'sNLP/SQL query's and from 3 Data, document, or web, there'ss an arrow labelled 'sResponse's
**üìÅ File Information:**
- **Filename:** AI_Agents_Integrate.png
- **Location:** scormcontent/assets/AI_Agents_Integrate.png
- **File Size:** 55.2 KB
- **File Type:** PNG
- **Dimensions:** 1417x801 pixels
- **Access:** Open SCORM zip ‚Üí scormcontent/assets/AI_Agents_Integrate.png

An AI agent in a robotic system might use cameras sensors and monitors to collect data about its environment, then run that information alongside software to make determinations about its next step. This is a process known as sensor fusion. Meanwhile, agentic AI in a software setting would collect data from other sources such as APIs online searches text prompts and databases that help the agents create a sense of perception and context.

---

### What can agentic AI do?

Generative AI applications have evolved rapidly over the last few years moving from simple use cases like text generation to more complex, multi-step workflows

### üñºÔ∏è Image: Image Content
**Alt Text:** Infographic builds across a series of images This is image 1 (of 3) Title Agentic AI Subtitle From passive tools to active decision makers 'sLevel 0, Repetitive tasks Follows simple predefined rules Use cases Chatbots for FAQs Level 1, Information Retrieval Agent, Retrieves data and recommends context-aware actions Use Cases Pulls from a knowledge base to recommend next steps for a support ticket
**üìÅ File Information:**
- **Filename:** AgenticAI_1.jpg
- **Location:** scormcontent/assets/AgenticAI_1.jpg
- **File Size:** 46.0 KB
- **File Type:** JPG
- **Dimensions:** 1680x829 pixels
- **Access:** Open SCORM zip ‚Üí scormcontent/assets/AgenticAI_1.jpg

### üñºÔ∏è Image: Image Content
**Alt Text:** Infographic builds across a series of images this is image 2 (of 3). It adds Levels 2 and 3 to the infographic. Level 2, Task Orchestration Agents Agents autonomously orchestrate low complexity tasks in a single data environment, can autonomously execute tasks Use Cases Schedules meetings and sends follow-up emails within a calendar/email system; Level 3, Muti-Domain Orchestration, Orchestrates workflows by connecting and harmonizing data across multiple business domains (ex. CRM or Finance), Use Cases Manages a sales pipeline using harmonized data from CRM, service tickets and financial reports
**üìÅ File Information:**
- **Filename:** AgenticAI_2.jpg
- **Location:** scormcontent/assets/AgenticAI_2.jpg
- **File Size:** 88.3 KB
- **File Type:** JPG
- **Dimensions:** 1680x843 pixels
- **Access:** Open SCORM zip ‚Üí scormcontent/assets/AgenticAI_2.jpg

### üñºÔ∏è Image: Image Content
**Alt Text:** Infographic builds across a series of images This is the third image (of 3) and adds Level 4 for the final piece of the graphic. Level 4, Multi-Agent Systems A system of specialized agents collaborate in real-time to manage complex, crossdomain processes Use Cases Specialized agents collaborate to process orders update inventory, and route customer feedback
**üìÅ File Information:**
- **Filename:** AgenticAI_3.jpg
- **Location:** scormcontent/assets/AgenticAI_3.jpg
- **File Size:** 106.1 KB
- **File Type:** JPG
- **Dimensions:** 1680x846 pixels
- **Access:** Open SCORM zip ‚Üí scormcontent/assets/AgenticAI_3.jpg

---

### Interactive Accordion

**Agentic AI is adaptive and dynamic.**
Agentic AI learns from previous patterns and data. This means that it can change its strategy based on new or changing information it receives in real time. While traditional workflows only move forward, agentic workflows can move forward and backward, with the ability to backtrack and mend errors as it goes In other words agentic AI can proactively anticipate needs and reflect on its own work.For example, an autonomous vehicle may use agentic AI to improve its ability to sense the difference between a piece of trash on the road and a squirrel. As it continuously monitors and analyzes its own behavior, it can improve the outcome of its actions

**Agentic AI can independently manage and complete tasks**
Agentic AI is sometimes referred to as autonomous AI. This is because it has the capability to communicate and collaborate with other AI systems and digital infrastructures on behalf of a human user, or another AI agent.For example, you can tell an AI agent that you want to make spaghetti for dinner. The AI agent could then complete the steps necessary to find a recipe, make a list of ingredients and place an order for those ingredients to be delivered to your home from a local grocery store.

**Agentic AI has a 'sschaining'ss ability.**
The AI system can perform a sequence of actions in response to a single request. For example, if you ask an AI agent to 'sscreate a website,'ss it can perform all the steps needed to carry out that task. This means that from one prompt, the AI agent can carry out the tasks of writing the code for the structure, populating the pages with content, designing the visuals and testing for responsiveness

### üñºÔ∏è Image: Image Content
**Alt Text:** 'sBy 2029, agentic AI will autonomously resolve 80 of common customer service issues without human intervention, leading to a 30 reduction in operational costs's
**üìÅ File Information:**
- **Filename:** By2029AgenticAI.jpg
- **Location:** scormcontent/assets/By2029AgenticAI.jpg
- **File Size:** 44.7 KB
- **File Type:** JPG
- **Dimensions:** 1252x567 pixels
- **Access:** Open SCORM zip ‚Üí scormcontent/assets/By2029AgenticAI.jpg

---

### What is an agentic workflow?

Agentic AI brings us closer to creating intelligent systems that can operate independently, collaborate effectively, and learn from their interactions with data. Agentic AI works because of a process known as an agentic workflow.

An agentic workflow is a structured series of actions managed and completed by AI agents

When an AI agent is given a goal to complete, it begins the workflow by breaking down a task into smaller individual steps then performing those stepsTo carry out this series of steps an AI agent spins up more versions of itself, creating a multi-agent system (MAS). In this workflow, the main agent (also known as a meta agent, orchestrator, or supervisor) delegates tasks to other agents assigning values and interacting with memory in a feedback loop. Together, the committee of agents work in parallel until the overall goal is complete.Within this MAS, each agent is made up of an internal structure that allows it to function both independently and collaboratively within its system. This collaboration is dependent on shared memory stores which provide context regarding individual knowledge, past experiences and belief states

---

### Benefits of agentic AI

Agentic AI is most useful for tasks that require continuous monitoring or rapid decision making. The benefits of agentic AI include:

Click each of the following flash cards to read more about each benefit of agentic AI.

### üÉè Flashcards: Interactive Flashcards

**Flashcard Set (3 cards):**

**Card 1:**
*Front:* More productivity
[Image: mountains.jpg]
*Back:* Delegating tasks to an AI agent allows for more focus to be placed on initiatives that add value to an organization. Think of it as an intern that works 24 hours a day, 7 days a week.
[Image: mountains.jpg]

---

**Card 2:**
*Front:* Reduced cost
[Image: mountains.jpg]
*Back:* Agentic AI reduces human error, taking away the cost associated with inefficiencies oversights and mistakes
[Image: mountains.jpg]

---

**Card 3:**
*Front:* Informed decision making
[Image: mountains.jpg]
*Back:* Agentic AI uses ML to filter and process massive amounts of real-time data. Insights from larger pools of good data result in better predictions and strategies
[Image: mountains.jpg]

---

### Agentic AI use cases

Agentic AI can be used for many purposes Let'sss take a look at how agentic AI may be used in an industry setting:Business operations could use an AI agent to manage supply chains optimize inventory levels forecast demands and plan logisticsHealthcare fields could use an AI agent to engage with clients monitor needs carry out treatment plans and provide personalized support.Software development could grow more efficient from using agentic AI to automatically generate debugging code, manage development lifecycle, and design system architecture.Software operations could use agentic AI for the autonomous operations of networks and other IT infrastructure or servicesCybersecurity could benefit from an AI agent helping to monitor network traffic, detect issues and respond to threats in real time.Researchers could use agentic AI to design and run experiments analyze data, formulate new hypotheses and generally speed up the pace of innovation by operating faster than a single human (or group of researchers ever could.Finance and trade could be enhanced by agentic AI'sss ability to constantly analyze market trends make trading decisions and adjust strategy based on streams of real-time data it has access to.

---

### Agentic AI vs generative AI

Both agentic AI and generative AI offer productivity benefits by assisting, augmenting, and optimizing tasks and processes Both are forms of AI that use LLMsWhen comparing the two, think of agentic AI as proactive and generative AI as reactive.Agentic AI is a system that can proactively set and complete goals with minimal human oversight. If part of accomplishing that goal involves creating content, generative AI tools handle that task. Agentic AI becomes an agent of the user and/or system.Generative AI is a tool that creates new content in reaction to a prompt. It can be a component of an agentic system, but it doesn'sst act on its own to complete a task. It has no agency.Agentic AI and generative AI do work collaboratively. Agentic AI systems may use generative AI to converse with a user, independently create content as part of a greater goal, or communicate with external tools generative AI is a critical part of agentic AI'sss 'sscognitive'ss process Both types of AI are adaptable in their own way. Generative AI expresses its adaptability by producing content in many styles and for different contexts Agentic AI showcases its adaptability by adjusting its plan and strategy in response to changing environmental conditions or new information.

---

### Let'ss consider a hypothetical use case.

Imagine a sales representative wants to use AI to write a follow-up email to a sales lead.With generative AI, the sales representative opens a generative AI interface and enters a prompt like, 'ssWrite a polite and professional follow-up email to Maria Wang about our proposal.'ss The generative AI produces a draft of the email and has fulfilled its purpose. It'sss now up to the sales representative to copy that text, paste it into an email, enter the recipient'sss email address and hit send.Now let'sss explore how agentic AI would handle a similar task.Within an agentic system, the sales representative sets a rule or command in their customer relationship management (CRM) system. It might read something like, 'ssFor any sales lead I mark as 'ssFollow-up required,'ss wait two business days then send a follow-up email.'ssOnce the sales representative marks Maria Wang as 'ssFollow-up required,'ss the agentic workflow is triggered. The system has its instructions (the initial prompt) and independently lays out a plan to enact with the help of external tools The plan may look something like this

### üìã Interactive Process

**Introduction:** The agentic workflow
Once the sales representative marks Maria Wang as 'ssFollow-up required,'ss the agentic workflow is triggered. The system has its instructions (the initial prompt) and independently lays out a plan to enact with the help of external tools The plan may look something like this

**Summary:** The end
Continue to the next topic.

**Step 3:** Process Step
After two business days the system sends a request to the agentic workflow.

**Step 4:** Process Step
The system retrieves Maria'sss details from the CRM.

**Step 5:** Process Step
Another tool fetches additional information about Maria (customer history, personalization details company information, etc.) that provides context for the prompt for the follow-up email.

**Step 6:** Process Step
The system creates a prompt for the follow-up email and provides it to an integrated gen AI model, which writes the email text.

**Step 7:** Process Step
The system provides a draft of the follow-up email to the sales representative, who approves the email or sends it back to redraft.

**Step 8:** Process Step
If the sales representative approves the email, the system makes an application programming interface (API) call to Maria'sss email service.

**Step 9:** Process Step
The system sends the email to Maria.

**Step 10:** Process Step
The system updates the CRM to show the email has been sent.

---

### What is Model Context Protocol (MCP)?

Model Context Protocol (MCP) is an open source protocol, introduced by Anthropic, that enables two-way connection and standardized communication between AI applications and external services

An open source protocol, or set of instructions is like a recipe for code that'sss freely available to use and contribute to.

MCP defines how models can call external tools retrieve data, and interact with services to enhance their capabilities It provides a simplified and reliable way for AI systems to virtually 'ssplug in'ss to different data sources and tools Think of MCP as a USB-C cable that connects devices to accessories and allows for transmission of data.

### How does MCP communication work?

### üñºÔ∏è Image: Image Content
**Alt Text:** Title Model Context Protocol (MCP) Subtitle The plug-and-play gateway to AI tools Left side of graphic consists of a diagram. A box labelled 'sMCP client's is on the far left. There are four arrows going from 'sMCP client's to 'sMCP serverss then there are four arrows running from each 'sMCP server's to 'sToolsdata's On the right side of the graphic is the following text: Open protocol that enables LLMs to use external tools and data, simplifying integration and expanding capabilities Simplified tooling: Allows AI systems to 'splug and play's with different tools without custom integrations Decoupled architecture: Separates the AI model from the specific tools it uses making the system more modular and easier to maintain Enhanced portability: Agents can run in any host environment that supports MCP and connect to any compliant server
**üìÅ File Information:**
- **Filename:** ModelContextProtocol.jpg
- **Location:** scormcontent/assets/ModelContextProtocol.jpg
- **File Size:** 92.2 KB
- **File Type:** JPG
- **Dimensions:** 1680x800 pixels
- **Access:** Open SCORM zip ‚Üí scormcontent/assets/ModelContextProtocol.jpg

MCP is based on the client-host-server model, also known as the client-server model, which includesMCP client: The AI application or system that requests access to external data or resourcesMCP host: The infrastructure (virtual machine, container, or serverless function) that manages communication between the client and server.MCP server: The component that provides specific tools resources and capabilities to the client.MCP begins with a handshake protocol. This initial greetingalso known as a capability discoveryconfirms that the MCP client and the MCP server can talk to each other.During the handshake protocol, the MCP client and the MCP server swap critical information to ensure they'ssre compatible. The client shares which capabilities it has and which version of MCP it understands In return, the server shares which capabilities it supports and which tools and resources it can provide to the client.

### MCP and agentic AI

MCP and agentic AI enable each other to create intelligent AI systems With MCP, AI systems can interact with the broader digital ecosystem to accomplish tasks for users Without MCP, agentic AI can think and plan (all the traits of generative AI), but can'sst interact with any outside systems

Although rapid adoption of MCP by major industry players may make it seem as though it will become the standard for AI tool integration, it is not the only option. Examples of other protocols include LangChain Agent Protocol and Google'ss Agent2Agent (A2A).

---

### Takeaways

Here are the takeaways from this lesson:

‚Ä¢ Agentic AI is an AI system designed to interact with data and tools in a way that requires minimal human intervention; agentic AI can accomplish tasks by creating a list of steps and performing them autonomously.

‚Ä¢ AI agent as an entity that sits on top of other software tools and operates them.

‚Ä¢ An agentic workflow is a structured series of actions managed and completed by AI agents

‚Ä¢ Think of agentic AI as proactive and generative AI as reactive.

‚Ä¢ Model Context Protocol (MCP) is an open source protocol, introduced by Anthropic, that enables two-way connection and standardized communication between AI applications and external services

---

### Knowledge check

### üß† Knowledge Check: True or false: An agentic workflow is a structured series of actions managed and completed by AI agents

**Answer Options:**
A. True ‚úÖ
B. False

**‚úÖ Correct Feedback:** True. Generative AI involves the use of deep learning models

---

### üß† Knowledge Check: Which of the following statements most accurately describes a characteristic of agentic AI?

**Answer Options:**
A. It can only follow simple, predefined rules
B. It exhibits autonomy, goal-driven behavior, and adaptability. ‚úÖ
C. It can predict the most likely of several potential outcomes but cannot generate new content.
D. It requires a minimum of four AI agents working in concert.

**‚úÖ Correct Feedback:** Correct! Agentic AI exhibits autonomy, goal-driven behavior, and adaptability.
**‚ùå Incorrect Feedback:** Incorrect. Please try again.

---


## AI Security and Using AI Responsibly 


### Lesson overview

Artificial intelligence (AI) brings enormous opportunities but also unique risks As AI adoption scales up, it increases risks for enterprises and individuals This lesson takes a high-level look at AI security and provides guidance on using AI responsibly.

### Lesson objectives

‚Ä¢ Define AI security and AI safety.

‚Ä¢ Describe examples of risks related to AI security and AI safety.

‚Ä¢ Explain, at a high level, steps organizations can take to mitigate risks related to AI security and AI safety

‚Ä¢ Summarize actions users can take to use AI safely and responsibly.

---

### AI security vs AI safety

### üñºÔ∏è Image: Image Content
**Description:** AI security and AI safety are related but distinct concepts Although it'ss necessary to consider and employ both to reduce risk, they address different challenges
**üìÅ File Information:**
- **Filename:** stock-image.jpg
- **Location:** scormcontent/assets/S-oN69/stock-image.jpg
- **File Size:** 453.4 KB
- **File Type:** JPG
- **Dimensions:** 1680x1680 pixels
- **Access:** Open SCORM zip ‚Üí scormcontent/assets/S-oN69/stock-image.jpg

### AI security

AI security focuses on protecting the confidentiality, integrity, and availability of AI systems The goal is to prevent malicious actors from attacking or manipulating those systems AI security involves protecting against threats that can arise from:Exploitation of vulnerabilities in software componentsMisconfigured systems or broken authentication allowing unauthorized access to sensitive enterprise data.Prompt injection (a type of security vulnerability or attack in which a user attempts to hijack or manipulate the AI model'ss instructions by sneaking in unauthorized or malicious commands within their input, that is the prompt).Model 'ssjailbreaking'ss (bypassing safety guardrails for example, ethical, legal, or content policies to generate restricted, harmful, or prohibited content).

### AI safety

In AI contexts safety means trustworthiness fairness and ethical alignment. AI safety helps maintain the intended behavior of AI systems keeping them aligned with company policies regulations and ethical standards The risks surrounding AI safety are less about system compromise and more related to what an AI system produces Issues related to AI safety include:Harmful or toxic language (for example, an AI model'ss responses could contain offensive or abusive content).Bias or discrimination in responsesHallucinations (plausible-sounding but false answers.Dangerous or misleading recommendations (for example, suggesting that users put glue on pizza or swallow drain cleaner to treat a sore throat).

---

### Detecting AI security threats

Organizations looking to protect their team and technology often layer their strategiesa single line of defense likely won'sst cut it. Common tactics include the following:

### Interactive Accordion

**Behavioral analysis**
This type of threat detection can catch anomalies and deviations within a network. After tracking typical datasets patterns and activity, it becomes intimately familiar with the AI system'sss typical behavior. When it comes across atypical behavior, such as biased contentor worse, public-facing passwords in cleartext formatit triggers an alert.

**Runtime threat detection**
If an adversary is scanning an environment for possible exploitations existing runtime security can detect repeated probes and sound the alarm. Security teams can automate and enhance this technique with AI to recognize threats and trigger alerts sooner.

**Predictive threat intelligence**
This technology anticipates future events by referencing historical data to make predictions about what will happen next. For example, if an adversary were targeting fintech systems with ransomware, predictive threat intelligence would assess the organizational posture against the likelihood of a successful attack.

**Enhanced data processing**
AI workloads contain a lot of datatypically, billions and billions of data points AI security has to process that data in order to conclude whether it'sss at risk, confidential, or available. Enhanced data processing can detect anomalies and threats in the environment faster than humans or traditional processing technology, so teams can act quickly.

**Attack path analysis**
This strategy involves mapping potential vulnerabilities and opportunities for threats For example, understanding how a threat may enter an organization'ss systems and reach sensitive data helps security teams identify which path an attack would come from and block it.

---

### AI security best practices

### üñºÔ∏è Image: Image Content
**Description:** Every phase of the AI lifecycle has vulnerabilities that need protection. Fortunately, there are practices in which organizations can engage to help mitigate such vulnerabilities
**üìÅ File Information:**
- **Filename:** stock-image.jpg
- **Location:** scormcontent/assets/4P7kBG/stock-image.jpg
- **File Size:** 129.9 KB
- **File Type:** JPG
- **Dimensions:** 1680x1120 pixels
- **Access:** Open SCORM zip ‚Üí scormcontent/assets/4P7kBG/stock-image.jpg

Elements of a healthy AI security strategy that can help protect an organization'ss AI models data, and privacy include the following best practices

### Interactive Accordion

**Adopting AI guardrails**
Guardrails help generative AI models filter hateful, abusive, or profane speech, personally identifiable information, competitive information, or other domain-specific constraints

**Protecting training data**
AI systems tend to be as reliable as the data they were trained on. Original training data should be secured behind firewalls or other safeguards against tampering or manipulation to protect the model'sss integrity and outputs

**Implementing strong platform security**
Protecting the platform on which AI workloads run can help ensure their health and reliability. If the platform is secure, threat actors have to work harder to inflict harm.

**Focusing on supply chain and systems security**
Organizations should adapt existing best practices in supply chain and systems security to cover AI workloads Just as traditional software supply chain security verifies the integrity of open source libraries AI supply chain security can account for the provenance and integrity of training data, pretrained models and third-party AI components

**Customizing strategies**
AI workloads are unique, rarely fitting into one-size-fitsall security solutions Security strategy should be tailored to an organization'ss individual AI workloads designs and data.

---

### Using AI responsibly

So what can you as an individual user do to mitigate risks associated with AI security and safety? And how can you use AI responsibly?

‚Ä¢ Don'st enter confidential or sensitive information: In general, unless you know otherwise, treat an AI chat window as a public forum. Do not paste any code, internal documents customer names financial figures or unreleased product details into a public-facing LLM (such as free versions of ChatGPT).

‚Ä¢ Assume all input is training data: Always assume that any information you input might be used to train future versions of the model, making it discoverable by others Only use anonymized, aggregated, or public data.

‚Ä¢ Use approved, internal tools Where available, use company-licensed or approved AI tools that guarantee inputs are not used for training and have strict data retention policies If you'sre unsure, ask your IT department.

‚Ä¢ Review all AI-generated output: Although LLM responses to prompts are typically stated in a confident manner, do NOT assume AI-generated output is factual or accurate. AI models can hallucinate (generate false, plausible-sounding information). Always crossreference all facts figures quoted statements citation information, and so forth with verified, trusted internal or external sources In addition, LLMs can sometimes include copyrighted material verbatim in their responses and using such content without the permission of the respective copyright holder could result in copyright infringement.

‚Ä¢ Maintain professionalism: Do not engage in inappropriate, biased, or inflammatory conversations with an AI chatbot. Likewise, do not prompt the AI to discuss or comment on internal company politics rival companies unreleased products or sensitive client matters Assume your prompts and conversations could be audited or made public and have a negative impact on your reputation (and that of your employer) if you engage with AI in an unprofessional manner.

‚Ä¢ Limit personal sharing: Avoid disclosing any private or sensitive personal details (e.g., home address personal schedules family names to public AI tools as this information can be harvested and used for targeted attacks against you or your colleagues

‚Ä¢ Be vigilant regarding social engineering attempts Malicious actors may use AI to craft highly convincing phishing emails deepfake voicemails or seemingly legitimate documents It'ss not enough anymore to look for poorly constructed or written emails Take care before clicking links especially in emails or messages you were not expecting.

‚Ä¢ Be skeptical toward any advice an AI may provide: If an AI offers advice on specialized areas (e.g., legal, medical, or financial), keep in mind that AI is no substitute for a licensed professional. Always consult a human expert in these domains

‚Ä¢ Be transparent: If you'sve used AI to generate content (e.g., code, text, images or videos, be clear about that with your audience. Include disclaimers or statements regarding AI use to clarify how you used AI and to what degree you used it.

---

### Thinking about your AI conversation

Of course, there'sss no way to prepare you for every possible AI-related conversation you might have. As always do your best to know your audience and tailor your conversation to their interests needs and expertise. Be ready to adapt if the conversation shifts and keep in mind that conversations involve two-way communication.

---

### Takeaways

Here are the takeaways from this lesson:

‚Ä¢ AI security and AI safety are related but distinct concepts

‚Ä¢ AI security focuses on protecting the confidentiality, integrity, and availability of AI systems The goal is to prevent malicious actors from attacking or manipulating those systems

‚Ä¢ In AI contexts safety means trustworthiness fairness and ethical alignment. AI safety helps maintain the intended behavior of AI systems keeping them aligned with company policies regulations and ethical standards

‚Ä¢ Common tactics for detecting AI security threats include behavioral analysis runtime threat detection, predictive threat analysis and enhanced data processing.

‚Ä¢ AI security best practices include adopting AI guardrails protecting training data, implementing strong platform security, focusing on supply chain and systems security, and customizing strategies to an organization'ss individual AI workloads designs and data.

‚Ä¢ Some tips for using AI responsibly include avoiding sharing sensitive or confidential information with public chatbots assuming any data shared with such a public LLM may be used to train that LLM, using IT-approved AI tools maintaining professionalism when using AI tools and being transparent with your audience in how you'sve used AI.

---

### Congratulations

You have completed the AI Fundamentals course!

If you are interested in more information about AI, check out the following resourcesA PDF compilation of key takeaways from each lesson in this course that you can download and consult as necessary.

### üì± Media: Media Content

An interactive PDF containing key terms related to AI, including links to videos containing more information.

### üì± Media: Media Content

An abbreviated AI timeline containing major landmarks in the history and development of AI.

### üì± Media: Media Content

---

---

## üìä Course Analysis

### üìÇ How to Access Course Assets

**To access the images and files listed below:**
1. Open the original SCORM zip file
2. Navigate to the `scormcontent` folder
3. Open the `assets` folder
4. All images, PDFs, and media files should be located in this folder

**Note:** Some files may be in subfolders within the assets directory. Use the file paths shown in the inventory below to locate specific files.

### üìÅ Asset Inventory
**Total Assets:** 47 files (4.78 MB)

**PDF Files (3):** 1076.2 KB
- scormcontent/assets/AI Fundamentals Key Terms.pdf (626.7 KB)
- scormcontent/assets/AI Timeline .pdf (372.8 KB)
- scormcontent/assets/AI Fundamentals_ Key Takeaways__.pdf (76.7 KB)

**JPG Files (31):** 3427.7 KB
- scormcontent/assets/S-oN69/stock-image.jpg (453.4 KB)
- scormcontent/assets/79AzwS/stock-image.jpg (204.0 KB)
- scormcontent/assets/Uettbr/stock-image.jpg (192.1 KB)
- scormcontent/assets/stock-image.jpg (184.6 KB)
- scormcontent/assets/Va3Gu4/stock-image.jpg (177.3 KB)
- scormcontent/assets/netflix-movie-may-2024-1714581478969.jpg (161.1 KB)
- scormcontent/assets/vE2HZM/mountains.jpg (156.4 KB)
- scormcontent/assets/MTPS-b/stock-image.jpg (151.7 KB)
- scormcontent/assets/nmmmtB/stock-image.jpg (140.6 KB)
- scormcontent/assets/_nM_yG/stock-image.jpg (129.9 KB)
- scormcontent/assets/4P7kBG/stock-image.jpg (129.9 KB)
- scormcontent/assets/AmazonCustomerReviewSummary.jpg (111.1 KB)
- scormcontent/assets/ImageOptimization.jpg (110.3 KB)
- scormcontent/assets/AgenticAI_3.jpg (106.1 KB)
- scormcontent/assets/TheBalancingActOfUsingFoundationModels.jpg (102.3 KB)
- scormcontent/assets/WhatIsAI_1a.jpg (98.4 KB)
- scormcontent/assets/AmazonRecs1a.jpg (93.0 KB)
- scormcontent/assets/ModelContextProtocol.jpg (92.2 KB)
- scormcontent/assets/WhatIsDeepLearning1a.jpg (90.7 KB)
- scormcontent/assets/AgenticAI_2.jpg (88.3 KB)
- scormcontent/assets/GoogleAIOverview.jpg (87.7 KB)
- scormcontent/assets/KfrlT7/stock-image.jpg (62.2 KB)
- scormcontent/assets/ReqsOfEnterpriseAIProductionSystems1.jpg (51.0 KB)
- scormcontent/assets/AgenticAI_1.jpg (46.0 KB)
- scormcontent/assets/By2029AgenticAI.jpg (44.7 KB)
- scormcontent/assets/Bank vault.jpg (36.5 KB)
- scormcontent/assets/jdWXSw/stock-image.jpg (36.2 KB)
- scormcontent/assets/UPSTruck.jpg (31.2 KB)
- scormcontent/assets/LEDbulb1.jpg (24.2 KB)
- scormcontent/assets/ChatGPT_3_5b.jpg (21.1 KB)
- scormcontent/assets/ChatGPT_3_5.jpg (13.5 KB)

**PNG Files (13):** 390.7 KB
- scormcontent/assets/AI_Agents_Integrate.png (55.2 KB)
- scormcontent/assets/ValueOfVLLM.png (52.4 KB)
- scormcontent/assets/WhatisML1a.png (51.0 KB)
- scormcontent/assets/GenerativeAI_2025.png (41.3 KB)
- scormcontent/assets/TextMessagePredictiveTexting.png (38.8 KB)
- scormcontent/assets/WhatIsInferencing.png (38.7 KB)
- scormcontent/assets/GoogleSearchBox.png (30.9 KB)
- scormcontent/assets/PredictiveAI_2025.png (28.7 KB)
- scormcontent/assets/KzeKOyXqF8Z-oBfd.png (21.8 KB)
- scormcontent/assets/vg-LNxOtAfx8aY_U.png (18.6 KB)
- scormcontent/assets/ChatGPT_logo.png (6.3 KB)
- scormcontent/assets/vLLM_Logo.png (3.8 KB)
- scormcontent/assets/tAU59QO0XmNElM9o.png (3.2 KB)



### üìà Content Type Breakdown
**Total Content Items:** 308
**Text Content:** 181 (58.8%)
**Video Content:** 0 (0.0%)
**Audio Content:** 0 (0.0%)
**Image Content:** 44 (14.3%)
**Interactive Content:** 10 (3.2%)
**Quiz/Assessment:** 11 (3.6%)
**Other Content:** 106 (34.4%)


### ‚è±Ô∏è Estimated Duration
**Estimated Total Duration:** 0h 60m
**Breakdown:**
- Text Content: 52.5 minutes (7868 words)
- Video Content: 7.5 minutes
- Audio Content: 0.0 minutes

**Per Lesson Breakdown:**
- **Introduction:** 9.8m (Text: 9.8m, Video: 0.0m, Audio: 0.0m)
- **Predictive AI and Generative AI:** 21.0m (Text: 21.0m, Video: 0.0m, Audio: 0.0m)
- **Inferencing and Optimization:** 7.6m (Text: 7.6m, Video: 0.0m, Audio: 0.0m)
- **Agentic AI:** 10.8m (Text: 10.8m, Video: 0.0m, Audio: 0.0m)
- **AI Security and Using AI Responsibly :** 10.8m (Text: 3.3m, Video: 7.5m, Audio: 0.0m)
